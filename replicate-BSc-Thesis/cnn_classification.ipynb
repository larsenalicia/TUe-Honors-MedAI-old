{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RtQaGf9xK5Gl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#import cv2\n",
        "import glob\n",
        "import pathlib\n",
        "import PIL, PIL.Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FkXd94xGNvHS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "current directory: /Users/alicialarsen/Documents/TUe/Honors/MedAI/replicate-BSc-Thesis\n",
            "datasets directory: /Users/alicialarsen/Documents/TUe/Honors/MedAI/replicate-BSc-Thesis/datasets/images\n"
          ]
        }
      ],
      "source": [
        "base_dir = os.getcwd()\n",
        "print(f'current directory: {base_dir}')\n",
        "\n",
        "dataset_dir = pathlib.Path(os.path.join(base_dir, 'datasets/images/'))\n",
        "print(f'datasets directory: {dataset_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of total images: 50 \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "image_count = len(list(dataset_dir.glob('**/*.png')))\n",
        "print('number of total images:', image_count, '\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50 files belonging to 2 classes.\n",
            "Using 40 files for training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50 files belonging to 2 classes.\n",
            "Using 10 files for validation.\n",
            "['a', 'b']\n"
          ]
        }
      ],
      "source": [
        "batch_size = 5\n",
        "img_height = 1108\n",
        "img_width = 1488\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(img_height, img_width),\n",
        "    batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  dataset_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(4):\n",
        "    ax = plt.subplot(2, 2, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 1108, 1488, 3)\n",
            "(5,)\n"
          ]
        }
      ],
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e938lrVNOTNM"
      },
      "source": [
        "####**CNN classification with a simple model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szCE6q72SrVq",
        "outputId": "fee6a91d-79cf-4731-a46c-a9a0eaeea392"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8/8 [==============================] - 120s 15s/step - loss: 0.6990 - accuracy: 0.4500 - val_loss: 0.6937 - val_accuracy: 0.4000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x17f2bf5e0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "simple_model = tf.keras.Sequential([\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.Conv2D(32, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, activation='relu'),\n",
        "  layers.Conv2D(64, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(128, 3, activation='relu'),\n",
        "  layers.Conv2D(128, 3, activation='relu'),\n",
        "  layers.Conv2D(128, 3, activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(512, activation='relu'),\n",
        "  layers.Dense(512, activation='relu'),\n",
        "  layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "simple_model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "simple_model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1N4YZkMObNC"
      },
      "source": [
        "####**CNN classification with a VGG-16 model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM9vcz_myyeA",
        "outputId": "70d686e0-40c1-4865-fefc-3551ef0b84e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'num_classes = len(class_names)\\n\\nvgg16_model = keras.Sequential()\\nvgg16_model.add(layers.Conv2D(input_shape=(1108, 1488, 3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\\nvgg16_model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\\nvgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\\nvgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\\nvgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\\nvgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\\nvgg16_model.add(layers.Flatten())\\nvgg16_model.add(layers.Dense(4096,activation=\"relu\"))\\nvgg16_model.add(layers.Dense(4096,activation=\"relu\"))\\nvgg16_model.add(layers.Dense(num_classes, activation=\"softmax\"))\\nvgg16_model.summary()\\n\\nopt = keras.optimizers.Adam(learning_rate=0.0001)\\nvgg16_model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[\\'accuracy\\'])\\n\\nvgg16_model.fit(train_ds, validation_data=val_ds, epochs=3)'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"num_classes = len(class_names)\n",
        "\n",
        "vgg16_model = keras.Sequential()\n",
        "vgg16_model.add(layers.Conv2D(input_shape=(1108, 1488, 3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
        "vgg16_model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "vgg16_model.add(layers.MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
        "vgg16_model.add(layers.Flatten())\n",
        "vgg16_model.add(layers.Dense(4096,activation=\"relu\"))\n",
        "vgg16_model.add(layers.Dense(4096,activation=\"relu\"))\n",
        "vgg16_model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "vgg16_model.summary()\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "vgg16_model.compile(optimizer=opt, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "vgg16_model.fit(train_ds, validation_data=val_ds, epochs=3)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CdSSYOS84gcH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/alicialarsen/anaconda3/envs/MedAI/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "models_path = os.path.join(base_dir, 'models')\n",
        "#vgg16_model.save(os.path.join(models_path, 'vgg16_model.h5'))\n",
        "simple_model.save(os.path.join(models_path, 'simple_model.h5'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M1MmOfj5fHI"
      },
      "source": [
        "use `model = keras.models.load_model('path/to/location')` to load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QDirj4T-M6XW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
          ]
        }
      ],
      "source": [
        "simple_model = keras.models.load_model(os.path.join(models_path, 'simple_model.h5')) \n",
        "#vgg16_model = keras.models.load_model(os.path.join(models_path, 'vgg16_model.h5')) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iMUhRbatNmG-",
        "outputId": "82ca9a67-7a5b-4158-9d5b-457afe846944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(simple_model, dpi=75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vpumo6EwNnq-",
        "outputId": "fb1ba3f3-f894-4011-e090-5fd56b45b6a9"
      },
      "outputs": [],
      "source": [
        "# tf.keras.utils.plot_model(vgg16_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX4yFef0OeVa"
      },
      "source": [
        "**Evaluations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu8DSB8p5rcg",
        "outputId": "9a03ddf4-a970-44c3-8094-3cad161e1289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 4s 2s/step - loss: 0.6937 - accuracy: 0.4000\n",
            "simple CNN's accuracy: 40.0%\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, name='model'):\n",
        "  loss, acc = model.evaluate(val_ds, batch_size= batch_size)\n",
        "  print(f\"{name}'s accuracy: {round((acc * 100), 2)}%\")\n",
        "\n",
        "evaluate(simple_model, 'simple CNN')\n",
        "# evaluate(vgg16_model, 'VGG-16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48DPvcIuIjLa",
        "outputId": "bacc4b71-d0ec-44ae-c0a2-aba171512404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 3s 1s/step\n"
          ]
        }
      ],
      "source": [
        "def predict(model):\n",
        "  return np.argmax(model.predict(val_ds), axis=-1)\n",
        "\n",
        "def get_labels(dataset, batch_count=2):\n",
        "  lst = []\n",
        "  for im, label in val_ds.take(2):\n",
        "    lst.extend(label.numpy())\n",
        "  return np.array(lst)\n",
        "\n",
        "# vgg16_comparison_list = list(zip(predict(vgg16_model), get_labels(val_ds)))\n",
        "simple_comparison_list = list(zip(predict(simple_model), get_labels(val_ds)))\n",
        "#print(\"VGG-16 (predictions, true labels): \", vgg16_comparison_list)\n",
        "#print(\"simple model (predictions, true labels): \", vgg16_comparison_list)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
